{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Graph CNN model from the DLTK models\n",
    "import graph_utils as utils\n",
    "from dltk.models.graphical.cgcnn import CGCNN\n",
    "\n",
    "# Set the CUDA_VISIBLE_DEVICES environmental variable to GPU ids to compute on\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "# Create a save path for log files and model parameters\n",
    "save_path =  '/tmp/MNIST_graph_cnn'\n",
    "shutil.rmtree(save_path, ignore_errors=True)\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Load the MNIST data via tf.examples\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('../../data/MNIST_data', one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Graph parameters\n",
    "coarsening_levels = 4\n",
    "number_edges = 8\n",
    "metric = 'euclidean'\n",
    "\n",
    "# Network parameters\n",
    "num_classes = max(mnist.train.labels) + 1  # number of classes\n",
    "\n",
    "filters = [32, 64]\n",
    "K_order = [25, 25]\n",
    "strides = [4, 4]\n",
    "num_fc = [512, num_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_graph(m, corners=False):\n",
    "    ''' DOCSTRING PLEASE! '''\n",
    "    z = utils.grid(m)\n",
    "    dist, idx = utils.distance_sklearn_metrics(z, k=number_edges, metric=metric)\n",
    "    A = utils.adjacency(dist, idx)\n",
    "\n",
    "    # Connections are only vertical or horizontal on the grid.\n",
    "    # Corner vertices are connected to 2 neightbors only.\n",
    "    if corners:\n",
    "        import scipy.sparse\n",
    "        A = A.toarray()\n",
    "        A[A < A.max()/1.5] = 0\n",
    "        A = scipy.sparse.csr_matrix(A)\n",
    "        print('{} edges'.format(A.nnz))\n",
    "\n",
    "    print(\"{} > {} edges\".format(A.nnz//2, number_edges*m**2//2))\n",
    "    return A\n",
    "\n",
    "A = grid_graph(28, corners=False)\n",
    "A = utils.replace_random_edges(A, 0)\n",
    "graphs, perm = utils.coarsen(A, levels=coarsening_levels, self_connections=False)\n",
    "L = [utils.laplacian(A, normalized=True) for A in graphs]\n",
    "del A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Transform data to a GCN compatible format\n",
    "train_data = mnist.train.images.astype(np.float32)\n",
    "test_data = mnist.test.images.astype(np.float32)\n",
    "test_labels = mnist.test.labels\n",
    "\n",
    "test_data = utils.perm_data(test_data, perm)\n",
    "\n",
    "# Build the GCNN network graph\n",
    "net = CGCNN(L, filters, K_order, strides, num_fc, bias='b1', pool='mpool', dropout=0.5)\n",
    "\n",
    "# Create placeholders to feed input data during execution\n",
    "batch_size = 100\n",
    "M_0 = L[0].shape[0]\n",
    "xp = tf.placeholder(tf.float32, (100, M_0), 'data')\n",
    "yp = tf.placeholder(tf.int32, (100), 'labels')\n",
    "\n",
    "# Compute the mean categorical crossentropy as a loss function\n",
    "logits_ = net(xp)['logits']\n",
    "labels_ = yp\n",
    "crossentropy_ = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_, labels=labels_)\n",
    "loss_ = tf.reduce_mean(crossentropy_, name='crossentropy')\n",
    "\n",
    "# Employ an ADAM optimiser to minimise the crossentropy loss during training\n",
    "train_op = tf.train.MomentumOptimizer(0.02, 0.9).minimize(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create additional ops to visualise the network output and track the training steps\n",
    "y_hat_ = net(xp, is_training=False)['y_']\n",
    "val_acc_ = tf.reduce_mean(tf.cast(tf.equal(tf.cast(yp, tf.int32), tf.cast(y_hat_, tf.int32)), tf.float32))\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "def predict(data, labels):\n",
    "    acc = 0\n",
    "    size = (data.shape[0])\n",
    "    \n",
    "    for begin in range(0, size, batch_size):\n",
    "        end = begin + batch_size\n",
    "        end = min([end, size])\n",
    "\n",
    "        batch_data = np.zeros((batch_size, data.shape[1]))\n",
    "        tmp_data = data[begin:end,:]\n",
    "        if type(tmp_data) is not np.ndarray:\n",
    "            tmp_data = tmp_data.toarray()  # convert sparse matrices\n",
    "        batch_data[:end-begin] = tmp_data\n",
    "\n",
    "        val_acc = s.run(val_acc_, {xp: batch_data, yp: batch_labels})\n",
    "        acc += val_acc\n",
    "\n",
    "    return acc * batch_size / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up a supervisor to continuously save and log the training progress, handle queues and initialise variables \n",
    "step = 0\n",
    "loss_moving = []  \n",
    "acc_moving = []  \n",
    "sv = tf.train.Supervisor(logdir=save_path,\n",
    "                         is_chief=True,\n",
    "                         summary_op=None,\n",
    "                         save_summaries_secs=30,\n",
    "                         save_model_secs=60,\n",
    "                         global_step=global_step)\n",
    "\n",
    "s = sv.prepare_or_wait_for_session(config=tf.ConfigProto())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training loop  \n",
    "while not sv.should_stop():\n",
    "    \n",
    "    # Get a batch of training input pairs of x (image) and y (label)\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    batch_data = utils.perm_data(batch[0], perm)\n",
    "    batch_labels = batch[1]\n",
    "    \n",
    "    # Run the training op and the loss\n",
    "    _, logits, loss = s.run([train_op, logits_, loss_], feed_dict={xp: batch_data, yp: batch_labels})\n",
    "    loss_moving.append(loss)    \n",
    "            \n",
    "    # Visualise all inputs, outputs and losses during each training step\n",
    "    if step % 20 == 0:\n",
    "        \n",
    "        # Compute the validation accuracy\n",
    "        val_acc = predict(test_data, test_labels)\n",
    "        acc_moving.append(val_acc)\n",
    "    \n",
    "        plt.close()\n",
    "        f, axarr = plt.subplots(1, 3, figsize=(16,4))\n",
    "        \n",
    "        axarr[0].imshow(np.reshape(batch[0], [-1, 28, 28])[-1], cmap='gray', vmin=0, vmax=1)\n",
    "        axarr[0].set_title('Input x; Prediction = {}; Truth = {};'.format(np.argmax(logits[-1,]), batch[1][-1,]))\n",
    "        axarr[0].axis('off')\n",
    "        \n",
    "        axarr[1].plot(loss_moving)\n",
    "        axarr[1].set_title('Training loss')\n",
    "        axarr[1].axis('on')\n",
    "        \n",
    "        axarr[2].plot(acc_moving)\n",
    "        axarr[2].set_title('Test acc')\n",
    "        axarr[2].axis('on')\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
