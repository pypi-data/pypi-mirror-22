{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%loadpy tutorial-part3.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import openpiv.tools\n",
      "import openpiv.process\n",
      "import openpiv.scaling\n",
      "\n",
      "frame_a  = openpiv.tools.imread( 'a.jpg' )\n",
      "frame_b  = openpiv.tools.imread( 'b.jpg' )\n",
      "\n",
      "# cause a.jpg and b.jpg are color images, one needs to convert them to greyscale\n",
      "# cause the image is too large and the flow is in a small region, we can crop it.\n",
      "\n",
      "u, v, sig2noise = openpiv.process.extended_search_area_piv( frame_a, frame_b, window_size=32, overlap=16, dt=0.02, search_area_size=64, sig2noise_method='peak2peak' )\n",
      "\n",
      "x, y = openpiv.process.get_coordinates( image_size=frame_a.shape, window_size=32, overlap=16 )\n",
      "\n",
      "u, v, mask = openpiv.validation.sig2noise_val( u, v, sig2noise, threshold = 1.3 )\n",
      "\n",
      "u, v = openpiv.filters.replace_outliers( u, v, method='localmean', max_iter=10, kernel_size=2)\n",
      "\n",
      "x, y, u, v = openpiv.scaling.uniform(x, y, u, v, scaling_factor = 1.0 )\n",
      "\n",
      "openpiv.tools.save(x, y, u, v, mask, 'tutorial-part3.txt' )\n",
      "\n",
      "openpiv.tools.display_vector_field('tutorial-part3.txt', scale=100, width=0.0025)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import openpiv.tools\n",
      "import openpiv.process\n",
      "import openpiv.scaling\n",
      "\n",
      "frame_a  = openpiv.tools.imread( 'a.jpg' )\n",
      "frame_b  = openpiv.tools.imread( 'b.jpg' )\n",
      "\n",
      "# cause a.jpg and b.jpg are color images, one needs to convert them to greyscale\n",
      "# cause the image is too large and the flow is in a small region, we can crop it.\n",
      "\n",
      "u, v, sig2noise = openpiv.process.extended_search_area_piv( frame_a, frame_b, window_size=32, overlap=16, dt=0.02, search_area_size=64, sig2noise_method='peak2peak' )\n",
      "\n",
      "x, y = openpiv.process.get_coordinates( image_size=frame_a.shape, window_size=32, overlap=16 )\n",
      "\n",
      "u, v, mask = openpiv.validation.sig2noise_val( u, v, sig2noise, threshold = 1.3 )\n",
      "\n",
      "u, v = openpiv.filters.replace_outliers( u, v, method='localmean', max_iter=10, kernel_size=2)\n",
      "\n",
      "x, y, u, v = openpiv.scaling.uniform(x, y, u, v, scaling_factor = 1.0 )\n",
      "\n",
      "openpiv.tools.save(x, y, u, v, mask, 'tutorial-part3.txt' )\n",
      "\n",
      "openpiv.tools.display_vector_field('tutorial-part3.txt', scale=100, width=0.0025)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "Buffer has wrong number of dimensions (expected 2, got 3)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-83-ac318cbf612f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# cause the image is too large and the flow is in a small region, we can crop it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig2noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenpiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextended_search_area_piv\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mframe_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_area_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig2noise_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'peak2peak'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenpiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_coordinates\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/alex/Library/Enthought/Canopy_32bit/User/lib/python2.7/site-packages/openpiv/process.so\u001b[0m in \u001b[0;36mopenpiv.process.extended_search_area_piv (openpiv/src/process.c:2330)\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: Buffer has wrong number of dimensions (expected 2, got 3)"
       ]
      }
     ],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import openpiv.tools\n",
      "import openpiv.process\n",
      "import openpiv.scaling"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "img_a  = openpiv.tools.imread( 'a.jpg' )\n",
      "img_b  = openpiv.tools.imread( 'b.jpg' )\n",
      "\n",
      "# cause a.jpg and b.jpg are color images, one needs to convert them to greyscale\n",
      "# cause the image is too large and the flow is in a small region, we can crop it."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "imshow(img_a)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frame_a = img_a[220:420,:,0]\n",
      "frame_b = img_b[220:420,:,0]\n",
      "imshow(frame_a,cmap=cm.gray)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "u, v, sig2noise = openpiv.process.extended_search_area_piv( frame_a, frame_b, window_size=32, overlap=16, dt=0.02, search_area_size=64, sig2noise_method='peak2peak' )\n",
      "\n",
      "x, y = openpiv.process.get_coordinates( image_size=frame_a.shape, window_size=32, overlap=16 )\n",
      "\n",
      "u, v, mask = openpiv.validation.sig2noise_val( u, v, sig2noise, threshold = 1.3 )\n",
      "\n",
      "u, v = openpiv.filters.replace_outliers( u, v, method='localmean', max_iter=10, kernel_size=2)\n",
      "\n",
      "x, y, u, v = openpiv.scaling.uniform(x, y, u, v, scaling_factor = 1.0 )\n",
      "\n",
      "openpiv.tools.save(x, y, u, v, mask, 'tutorial-part3.txt' )\n",
      "\n",
      "# openpiv.tools.display_vector_field('tutorial-part3.txt', scale=100, width=0.0025)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "Buffer has wrong number of dimensions (expected 2, got 3)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-84-2b430a593fe9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig2noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenpiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextended_search_area_piv\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mframe_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_area_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig2noise_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'peak2peak'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenpiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_coordinates\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenpiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msig2noise_val\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig2noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.3\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/alex/Library/Enthought/Canopy_32bit/User/lib/python2.7/site-packages/openpiv/process.so\u001b[0m in \u001b[0;36mopenpiv.process.extended_search_area_piv (openpiv/src/process.c:2330)\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: Buffer has wrong number of dimensions (expected 2, got 3)"
       ]
      }
     ],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ax = axes()\n",
      "quiver(x,y,u,v,u**2+v**2)\n",
      "axis('tight')\n",
      "ax.set_aspect(2.)\n",
      "f = gcf()\n",
      "f.set_size_inches(16,8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}