{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing external tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complex workflows usually have a large number of steps, most of them are light-weight and can be executed locally, but some of them can be time- and resource-consuming and would better be executed on dedicated servers or cluster systems. It can be challenging to execute such workflows efficiently, for example,\n",
    "\n",
    "1. Executing the entire workflow on the cluster as a single multi-processing job is not ideal because 1). you would have to allocate enough resource for the most CPU and RAM-demanging step and longest execution time for the most time-consuming step but these resources are not utilized efficiently, and 2). you are limiting yourself to a single node and cannot really execute the workflow in parallel.\n",
    "\n",
    "2. Separate the workflow by steps and submit them as separate tasks to a queuing system is much more efficient. This approach allows better parallelization but it can be difficult and time consuming to execute a large number of small jobs, because these tasks might spend more time waiting than running. In addition, it can be difficult to disect a workflow to be executable this way while maintaining all the dependencies. Also, running a workflow on a cluster system requires a running server (task dispatcher, corrdinator) to corrdinate the execution of dependent steps. This has posed a problem, at least to our clusters because such processes are not allowed on the head node.\n",
    "\n",
    "To address these problems, SoS taks a different approach than mose other workflow systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## SoS' task model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Complete workflow\n",
    "\n",
    "SoS defines workflows as steps to execute certain procedures on input files (forward-style), or as steps to produce desired outputs (makefile style). **All steps, regardless of their sizes (requirement for resources), are written as a single workflow**. That is to say, a long running sequence alignment step and the relatively quick statistical analysis steps would be defined as a single workflow.\n",
    "\n",
    "SoS executes all steps directly as a complete workflow. The workflow is executed in a multi-processing manner where multiple processes (by default to 4) are used to execute different branches of the DAG (Direct Acyclic Graph) in parallel.\n",
    "\n",
    "### Generation of external tasks\n",
    "\n",
    "Part of the steps can be defined as tasks that are executed externally. SoS tasks have the following properties:\n",
    "\n",
    "* **Tasks are self-contained** in that they contain all the required information to be executed anywhere. \n",
    "* **Tasks are generated and executed dynamically** in that a task would only be generated when all its dependencies have been met.\n",
    "* **Tasks are independent of workflows and other tasks**. Tasks are defined by the task they are executing. Tasks can be shared by different workflows if they happen to perform exactly the same function.\n",
    "* **Tasks are file system independent** in the sense that sos automatically synchronize input and output of tasks to remote systems so you can easily switch from a remote server to another, or to a cluster system.\n",
    "\n",
    "### Remote execution of tasks\n",
    "\n",
    "SoS tasks can be executed locally as separate  processes, remotely on a remote server, sent to distributed task-queues (such as [rq](http://python-rq.org/) or [Celery](http://www.celeryproject.org/), or cluster systems based on PBS, Torch, or SunGrid. SoS handles file synchronization so **tasks could be submitted to queues with their own file systems**. \n",
    "\n",
    "### Stop-and-resume execution of workflow\n",
    "\n",
    "SoS can wait for the completion of the tasks as if they are just long-running jobs executed locally. Alternatively, sos could exit and wait for the completion of external tasks. Command `sos status` or task-queue-specific methods could be used to monitor the execution of tasks.\n",
    "\n",
    "After the completion of the tasks, you could rerun the workflow and SoS would resume execution from the point where it was stopped. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Flexible design of workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "This external execution model offers great flexibility in the execution of workflows. For example,\n",
    "* You can execute a step on a remote server with more resource by specifying parameter `queue` of a single task.\n",
    "* You can submit all tasks of a workflow to a cluster by specifying cluster name with the `-q` option of command `sos run`.\n",
    "* You can submit part of the tasks to one machine, and part of the tasks to another task queue using the combination of task-specific option and global `-q` option.\n",
    "* You can use SoS as a task-generation tool to generate a bunch of tasks, and send the tasks to different computer systems for execution. SoS automatically handles file synchronization so that you can easily move from one cluster to another cluster or another server.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "The following figure illustrates the task model of SoS\n",
    "\n",
    "![job queue](../media/job_queue.svg )\n",
    "\n",
    "Basically,\n",
    "1. Tasks are part of step processes.\n",
    "2. Tasks are managed by task engines, multiple task engines can be used for a single workflow.\n",
    "3. Task engines generate task files, submit tasks, monitor task status, and return results to SoS workflow.\n",
    "4. Remote task engines synchronize input files, translate and copy tasks to server, and start the tasks on the remote server. Pre-defined `submit_cmd` could be used to submit tasks to batch systems such as PBS/Moab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Specification of tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **step process** consists of everything after the `input` statement. It can be repeated with different **input groups** defined by input options `group_by` or `for_each`. For example, if `bam_files` is a list of bam files,\n",
    "\n",
    "```\n",
    "[10]\n",
    "input: bam_files, group_by=1\n",
    "output: \"${_input}.bai\"\n",
    "\n",
    "run:\n",
    "    samtools index ${_input}\n",
    "```\n",
    "\n",
    "execute a shell script to process each bam file. This is done sequentially for each input file, and is performed by SoS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily specify part of the step process as **tasks**, by prepending the statements with a `task` keyword:\n",
    "\n",
    "\n",
    "```\n",
    "[10]\n",
    "input: bam_files, group_by=1\n",
    "output: \"${_input}.bai\"\n",
    "\n",
    "task:\n",
    "run:\n",
    "    samtools index ${_input}\n",
    "```\n",
    "\n",
    "This statement declares the rest of the step process as a `task`. For each input file, a task will be created with an ID determined from task content and context (input and output files, variables etc). The task will be by default executed by a `process` task queue where tasks are started as background processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit of executing tasks externally is that the tasks are executedconcurrently, on the local machine or a remote server, or be submitted to a task queue. For example, in the previous example, multiple tasks could be executed in parallel (but on the same machine) unless you specify it otherwise as follows\n",
    "\n",
    "```\n",
    "[10]\n",
    "input: bam_files, group_by=1\n",
    "output: \"${_input}.bai\"\n",
    "\n",
    "task: concurrent=False\n",
    "run:\n",
    "    samtools index ${_input}\n",
    "```\n",
    "\n",
    "You can also use command\n",
    "\n",
    "```\n",
    "sos run myscript -q cluster\n",
    "```\n",
    "or use option `queue`\n",
    "```\n",
    "[10]\n",
    "input: bam_files, group_by=1\n",
    "output: \"${_input}.bai\"\n",
    "\n",
    "task: queue='cluster'\n",
    "run:\n",
    "    samtools index ${_input}\n",
    "```\n",
    "\n",
    "to submit the commands to a cluster system to be executed on different computing nodes.\n",
    "\n",
    "This document focuses on the configuration of servers and the use of task options to execute tasks on different types of servers or task queues. It provides a comprehensive list of options while the [Remote Execution tutorial](../tutorials/Remote_Execution.html) gives a more step-by-step tutorial on how to configure and use the remote execution features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Configure `hosts`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "A server or task queue needs to be defined in SoS configuration file. SoS automatically loads global configuration files `~/.sos/hosts.yml`, `~/.sos/config.yml`, and a local configuration file `./config.yml`. You can also put your settings in any configuration file and specify it with option `-c`. `~/.sos/hosts.yml` is designed to record all host-related configurations and should be used for this purpose.\n",
    "\n",
    "The configuration file could be edited manually if you are familiar with the YAML format. Otherwise you can use the `sos config` command to add or modify settings. For example, command\n",
    "\n",
    "```bash\n",
    "sos config --hosts --set hosts.shark.address username@shark.com\n",
    "```\n",
    "\n",
    "would write to `~/.sos/hosts.yml` the following content\n",
    "\n",
    "```\n",
    "hosts:\n",
    "    shark:\n",
    "        address: username@shark.com\n",
    "```\n",
    "\n",
    "This effectively defines a host with alias `shark`. All configurations related to this host should be defined under `shark`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Define `localhost`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "You will need to configure both localhost and server to be able to send local jobs to remote servers or task queues. A `localhost` should be defined to point to an existing host definition. For example, if a `desktop` is defined in `~/.sos/hosts.yml` as \n",
    "\n",
    "```\n",
    "hosts:\n",
    "    OfficeDesktop:\n",
    "        paths:\n",
    "            home: /Users/myuser\n",
    "```\n",
    "\n",
    "You should use command\n",
    "\n",
    "```\n",
    "sos config --global --set localhost OfficeDesktop\n",
    "```\n",
    "\n",
    "to define `localhost` as `OfficeDesktop`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `address`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "IP address or URL of the host. Note that\n",
    "\n",
    "* `address` should be ignored for hosts, for example your desktop, that do not accept remote execution.\n",
    "* If you have a different user name on the the remote host, specify the `address` in the format of `username@hostaddress`.\n",
    "* SoS does not support username/password authentication and you will have to set up public key authentication between local and remote hosts to communicate with remote host."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `port`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "ssh port of the remote host, which is `22` by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `shared`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Option `shared` tells SoS which file systems are shared between localhost and some remote hosts so that it does not have to synchronize files under these directories between the hosts.\n",
    "\n",
    "The `shared` entry should be defined with `name` and `path` pairs in the format of\n",
    "\n",
    "```\n",
    "hosts:\n",
    "    desktop:\n",
    "    server:\n",
    "        shared:\n",
    "            project: /myprojects\n",
    "            HTFS: /\n",
    "    worker:\n",
    "        shared:\n",
    "            HTFS: /\n",
    "    server1:\n",
    "        shared:\n",
    "            project: /scratch/myprojects\n",
    "            data: /scratch/data\n",
    "````\n",
    "\n",
    "The above cooked configuration says:\n",
    "\n",
    "1. `desktop` does not share any volume with any other machine so all files need to be transferred.\n",
    "2. `server` and `worker` shares `HTFS` with directory `/`, so all files are shared.\n",
    "3. `server` and `server1` share a `project` volume but the volume is mounted at different locations. So files under `myprojects` are not synchronized if you are submitting jobs from `server` to `server1`, and files under `/scratch/myprojects` are not synchronized if you are submitting jobs from `server1` to `server`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `paths`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "`paths` defines paths that will be translated when a task is executed remotely. For example, your input file on a mac might be `/Users/myuser/project/KS28.fa`, but it should be named `/home/myuser/project/KS28.fa` if it is processed on a remote server. In this case, you should define directories `/Users/myuser` and `/home/myuser` as equivalent directories on the two hosts, using \n",
    "\n",
    "```\n",
    "hosts:\n",
    "    desktop:\n",
    "        paths:\n",
    "            home: /Users/myuser\n",
    "    server:\n",
    "        paths:\n",
    "            home: /home/myuser\n",
    "```\n",
    "\n",
    "Multiple entried could be defined and the files would be mapped by the longest mapping path. For example, if you have on the server a shared location for all resources, you could define\n",
    "\n",
    "```\n",
    "hosts:\n",
    "    desktop:\n",
    "        paths:            \n",
    "            home: /Users/myuser\n",
    "            resource: /Users/myuser/resources\n",
    "    server:\n",
    "        paths:\n",
    "            home: /home/myuser\n",
    "            resource: /shared/resources\n",
    "```\n",
    "\n",
    "so that `/Users/myuser/resources/hg19.fa` could be mapped to `/shared/resources/hg19.fa` on the server. Note that `/Users/myuser/resource/hg19.fa` would be matched to `resource` instead of `home` because `resource` matches longer piece of the input path.\n",
    "\n",
    "Two hosts must have the same set of `paths` to be connected so although sets of paths can vary (e.g. `home`, `scratch`, `working`, `resource`), it is important to define the same set of `paths` for all servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `path_map` (derived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "With definitions of `paths` on both hosts, SoS would derive a set of `path_maps` between all hosts. Actually, when you run \n",
    "\n",
    "```\n",
    "sos status -q\n",
    "```\n",
    "\n",
    "to list all host configurations, SoS would list all hosts accessible from `localhost`, with host-specific `path_map`, which is a list of directory mappings between local and remote directories. For example, the `path_map` from `desktop` to `server` using the above example would be\n",
    "\n",
    "```\n",
    "/Users/myuser:/home/myuser\n",
    "/Users/myuser/resoruces:/shared/resources\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Common queue configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `queue_type`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Option `query_type` determines the type of remote server or job queue. SoS currently supports the following types of job queues:\n",
    "\n",
    "1. **`process`**: this is the default queue type. Tasks are executed directly, either on local host or on a server.\n",
    "2. **`pbs`**: A PBS/MOAB cluster system where tasks are submitted using commands such as `qsub`.\n",
    "3. **`rq`**: A redis queue where tasks are submitted to the rq server and monitored through rq-dashboard.\n",
    "4. **`celery`**: A celery queue where tasks are submitted to the celery server and monitored through celery's flower module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `wait_for_task`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Whether or not SoS should wait for the completion of tasks (`true/false`). By default SoS will wait for the completion of tasks submitted in a queue and complete the workflow in one run. Alternatively, SoS would quit after all tasks have been submitted to a queue with `wait_for_task` set to `false`. The workflow can be resumed after tasks have been completed.\n",
    "\n",
    "This option is set to `True` by default and can be overridden by command line option `-w` (wait) or `-W` (no wait)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `status_check_interval`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Option `status_check_interval` determines frequency at which SoS checks status of jobs. This is set by default to 2 seconds for `process` queue type, and `10` seconds for all other types. This number should be set to at least `60` for remote servers and longer jobs because it can be a burden to query the status of jobs very frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `max_running_jobs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Maximum number of running jobs. This setting controls how SoS releases tasks to job queues and is independent of possible maximum running job settings of individual task queues.\n",
    "\n",
    "This option is set to 10 by default and can be overriden by command line option `-J`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `max_cores`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum number of processes (an integer) allowed for the queue. A task would fail immediately if it specifies more `cores` than this option. For queues that do not require a `cores` option (e.g. a `process` queue), tasks that use more processes than `max_cores` will be automatically killed.\n",
    "\n",
    "This option, along with `max_walltime` and `max_mem` that are described below, help prevent the submission of erraneous tasks to the queue. It also helps prevent the execution large jobs on wrong servers, such as the headnode of a cluster system on which only job preparation and submission steps are allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `max_walltime`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Maximum walltime allowed for the queue. `max_walltime` could be specified as an integer (seconds) or a string in the format of `HH:MM:SS`. A task would fail immediately if it specifies more `walltime` than this option. For queues that do not require a `walltime` option (e.g. a `process` queue), tasks that has exceeded `max_walltime` will be automatically killed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `max_mem`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum memory comsumption allowed for the queue. `max_mem` could be specified as an integer (bytes) or a string with units such as `M`, `G`, `MB` etc. A task would fail immediately if it specifies more `mem` than this option. For queues that do not require a `mem` option (e.g. a `process` queue), tasks that use more RAM than `max_mem` will be automatically killed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `description`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "An optional short description for the queue that will be displayed with commands such as `sos status -q`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Arbitrary key value pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "You can define arbitrary key value pairs in the host configuration. These variables could be used for the interpolation of commands and templates. For example, if you define\n",
    "\n",
    "```yml\n",
    "queue: long\n",
    "```\n",
    "\n",
    "You could use \n",
    "\n",
    "```bash\n",
    "#PBS -q ${queue}\n",
    "```\n",
    "\n",
    "in your PBS job templates (configuration `job_template`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## RQ configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `redis_host`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Address of the redis server, default to `localhost`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `redis_port`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Port of the redis server, default to `6379`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Celery configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `broker`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "output_cache": "[]"
   },
   "source": [
    "`broker` configuration of celery app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `backend`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "output_cache": "[]"
   },
   "source": [
    "`backend` configuration of the celery app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## PBS/Torch configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `job_template`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Option `job_template` is a template of a shell script that will be submitted to the PBS system. A typical template would be specified as (using a multi-line string literal of YAML format)\n",
    "\n",
    "```bash\n",
    "hosts:\n",
    "  server:\n",
    "    job_template: |\n",
    "      #!/bin/bash\n",
    "      #PBS -N ${job_name}\n",
    "      #PBS -l nodes=1:ppn=${cores}\n",
    "      #PBS -l walltime=${walltime}\n",
    "      #PBS -l mem=${mem//10**6}MB\n",
    "      #PBS -o ${cur_dir}/${task}.out\n",
    "      #PBS -e ${cur_dir}/${task}.err\n",
    "      #PBS -q long\n",
    "      #PBS -m ae\n",
    "      #PBS -M your@email.address\n",
    "      #PBS -v ${cur_dir}\n",
    "      \n",
    "      cd ${cur_dir}\n",
    "      \n",
    "      sos execute ${task} -v ${verbosity} -s ${sig_mode} \\\n",
    "        ${'--dryrun' if run_mode == 'dryrun' else ''}\n",
    "```\n",
    "\n",
    "The template will be interpolated with the following information\n",
    "\n",
    "* `task`: task id\n",
    "* `job_name`: Interpolated value of runtime option `name`, or task id if `name` is not provided. For example, `name='${step_name}_${_index}'` would assign names with step name and index to submitted jobs.\n",
    "* `nodes`, `cores`, `walltime`, `mem`: resource task options\n",
    "* `cur_dir`:  current project directory, which will be translated to path in remote host if the task is executed remotely\n",
    "* `verbosity` and `sig_mode`: sos run mode.\n",
    "* `run_mode` to allow the script to be executed in dryrun mode, in which mode scripts would be printed instead of executed. It is very important to set this option because the job script would be executed directly (on head node) instead of sent to the PBS queue if sos is running in dryrun mode (`sos run -q pbs -n`).\n",
    "* Other key/value pairs you defined for the server\n",
    "\n",
    "Note that\n",
    "1. You will need to specify resource options (`nodes`, `cores`, `walltime`, and `mem`) as task options if they are used in the template file.\n",
    "2. If you need to specify more options (e.g. queue name), you will have to define multiple host entries with different options. For example, you could define two queue entries as `cluster-short` and `cluster-long` for two queues on the same cluster. These two entries could share the same template file but different `queue: name` values.\n",
    "3. An useful feature is that SoS string interpolation accepts arbitrary Python expressions so it is possible to specify `queue` by `walltime`, using expressions such as \n",
    "\n",
    "    ```\n",
    "    ${'long' if walltime > 60*60*24*5 else 'short'}\n",
    "    ```\n",
    "\n",
    "    Note that walltime would be an integer internally even if it is specified in the format of `HH:MM:SS` in the script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `submit_cmd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "A `submit_cmd` template is the command that will be executed to submit the job. It accepts the same set of variables as `job_template`, with an additional variable `job_file` pointing to the location of the job file on the remote host. The `submit_cmd` is usually as simple as\n",
    "\n",
    "```bash\n",
    "qsub ${job_file}\n",
    "```\n",
    "\n",
    "but you could specify some options from command line instead of the job file and define `submit_cmd` as\n",
    "\n",
    "```bash\n",
    "msub -l ${walltime} < ${job_file}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `submit_cmd_output`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "This option specifies the output of the `submit_cmd` command and let SoS know how to extract `job_id` and other information from it. For example, for a regular PBS system, the output is simply the `job_id` (stripping spaces and newlines).\n",
    "\n",
    "```\n",
    "submit_cmd_output='{job_id}'\n",
    "```\n",
    "\n",
    "On a LSF system, the output should be similar to\n",
    "\n",
    "```\n",
    "submit_cmd_output='Job <{job_id}> is submitted to queue <{queue}>'\n",
    "```\n",
    "\n",
    "The information extracted (namely variables defined) from `submit_cmd` can be used for other commands such as `status_cmd`. Note that this parameter uses pattern matching `{ }` to extract variable, instead of `${ }` to use variables.\n",
    "\n",
    "This option is defult to `{job_id}`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `status_cmd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "An command to query the status of a submitted task. For a standard PBS system, this option could be\n",
    "\n",
    "```\n",
    "qstat ${job_id}\n",
    "```\n",
    "\n",
    "where `job_id` is the output of command `submit_cmd`. The `status_cmd` is interpolated with variables `job_id` (PBS job ID), `task` (SoS task id), and `verbosity` (command line verbosity level) so you would adjust options for different verbosity level (e.g. `${'-f' if verbosity > 2 else ''}`).\n",
    "\n",
    "Note that the `status_cmd` is only called with `-v 2` or higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `kill_cmd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "A command to kill a submitted job on the cluster. For a standard PBS system, this option could be\n",
    "\n",
    "```\n",
    "qdel ${job_id}\n",
    "```\n",
    "where `job_id` is the output of command `submit_cmd`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Resource options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "The resource options will be sent to individual task queues in appropriate format. You do not have to specify all options because task queues can support a subset of these options and some task queues provide default values (and some do not). It is however generally a good idea to specify them all so that your tasks could be executed on all types of task queues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `walltime`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Estimated maximum running time of the task. This parameter will be sent to different task queues and it is up to the task queue to decide if the task would be killed if the task could not be completed within specified `walltime`. `walltime` could be specified\n",
    "\n",
    "1. As an integer number (in seconds).\n",
    "2. As a string in the format of `HH:MM:SS` where `HH`, `MM` and `SS` are hours, minutes, and seconds. For example, you could use `walltime=240:00:00` for a job that would run 10 days.\n",
    "\n",
    "SoS automatically converts this option to an integer and formats it appropriately for different task engines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `cores`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Number of cores on each computing node, which corrsponds to the `ppn` option of a PBS system.\n",
    "\n",
    "PBS task queue also accepts a parameer `nodes` (corresponds to PBS resource option `nodes`, default to 1) but it is currently unused because SoS does not yet support multi-node tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `mem`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "The total amount of memory needed across all nodes. Default units are bytes; can also be expressed in megabytes (`mem=4000MB`). gigabytes (`mem=4GB`) or gibibytes (`mem=4GiB`), although all inputs are converted to bytes internally. To use this option in a `job_template`, you generally need to use expressions such as `${mem//1e9}GB`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Execution options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `name`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Name of the task, which can be a string such as `'${step_name}_${_index}'` that would be interpolated when a task is generated. The value of this parameter has to be single quoted because otherwise the string would be interpolated before it is passed to task engine.\n",
    "\n",
    "Name of task is currently only used to produce variable `job_name` for job template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `queue`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Option `queue` specifies a task queue to which the current task will be submitted. This option overrides system default (command line option `-q`) so it is generally a good idea to use command line option `-q` so that the task could be submitted to different task queues, unless the task has to be executed in a particular server (e.g. with a software that is unavailable elsewhere)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `preserved_vars`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "All variables in the task context environment, including implicit SoS variables such as `_input` and `_output` are automatically translated if they are string or sequence of strings (list, tuple etc). This would however translate variables such as `sample_name` with value `my_sample` to something like `/home/myuser/project/my_sample`. It is therefore important for you to identify all variables that should be preserved during context-switching in the option `preserved_vars`.\n",
    "\n",
    "This variables takes a list of variable names, but a string can be specified if there is only one name. That is to say, both\n",
    "\n",
    "```\n",
    "preserved_vars='sample_name'\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```\n",
    "preserved_vars=['sample_name', 'title']\n",
    "```\n",
    "\n",
    "are acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option `workdir`\n",
    "\n",
    "Default to current working directory.\n",
    "\n",
    "Option `workdir` controls the working directory of the task. For example, the following step downloads a file to the `resource_dir` using command `wget`.\n",
    "\n",
    "```python\n",
    "[10]\n",
    "\n",
    "task: workdir=resource_dir\n",
    "\n",
    "run:\n",
    "  wget a_url -O filename\n",
    "```\n",
    "\n",
    "Runtime option `workdir` will be translated to remote host if the task is executed remotely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option `concurrent`\n",
    "\n",
    "Default to `True`.\n",
    "\n",
    "If the step process is repeated for multiple input groups (using input options `group_by` or `for_each`), all loop processes will by default be sent to the task engine to be executed in parallel (subject to `max_running_jobs` of individual task queue). If your tasks are sequential in nature (e.g. the next input group depends on the result of the current input group), you can set `concurrent=False`, in which case the next task will be generated and sent to the task queue only after the current one has been completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option `env`\n",
    "\n",
    "The `env` option allow you to modify runtime environment, similar to the `env` parameter of the `subprocess.Popen` function. For example, you can execute your command with in a specific directory using\n",
    "\n",
    "```sos\n",
    "task:  env={'PATH': '/path/to/mycommand' + os.sep + os.environ['PATH']}\n",
    "run:\n",
    "   mycommand \n",
    "```\n",
    "\n",
    "Option `env` is NOT translated to remote host because it is of type directionay. The job template is usually a good place to set host-specific environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option `prepend_path`\n",
    "\n",
    "Option `prepend_path` is a shortcut to option `env` to prepend one (a string) or more (a list of strings) paths to system path. For example, the above example can be shortened to\n",
    "\n",
    "```sos\n",
    "task:  prepend_path='/path/to/mycommand'\n",
    "run:\n",
    "   mycommand \n",
    "```\n",
    "\n",
    "Option `prepend_path` is NOT translated to remote host because it is likely to be host specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `active`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Option `active` specifies the active task within a input loop. It should be an index or a list of indexes when the task will be executed. Negative index is acceptable (e.g. task for only the last input loop will be executed with `active=-1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Commands and Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `-q`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Option `-q` can be used by all task-related commands. It specifies a remote server or task queue, with detailed information defined in either global (`~/.sos/config.yml`) or local (`./config.yml`) configuration files. You can also save configurations to other configuration files and specify them using option `-c`. E.g.\n",
    "\n",
    "```bash\n",
    "sos run myscript -q shark -c shark.yml\n",
    "```\n",
    "uses definition of `shark` defined in `shark.yml`. You can define hosts in any of the configuration files and definitions loaded later override previous definitions. For example, you could define a set of test servers that is identical to definitions in global definition, but with `max_running_jobs` set to 1. You can then use\n",
    "\n",
    "```\n",
    "sos run myscript -q shark -c test.yml\n",
    "```\n",
    "for test runs and \n",
    "```\n",
    "sos run myscript -q shark\n",
    "```\n",
    "for regular execution.\n",
    "\n",
    "It is also possible for you to specify a host without configuration. Such hosts have default values\n",
    "\n",
    "```\n",
    "alias = host\n",
    "address = host\n",
    "queue_type = process\n",
    "path_map = []\n",
    "shared = []\n",
    "```\n",
    "\n",
    "which assumes that the host is a remote machine with identical but not shared file systems and without any task queue.\n",
    "\n",
    "Finally, if option `-q` is specified without value, the `sos` command will exit with a list of configured hosts so you can use command\n",
    "\n",
    "```\n",
    "sos status -q\n",
    "```\n",
    "to list all configured queued. Details of each queue will be displayed with option `-v3` or `-v4`.\n",
    "\n",
    "```\n",
    "sos status -q -v3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `sos run -q`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "The `-q` option of command `sos run` (or `sos-runner`) sets the default task queue for all tasks. For example,\n",
    "\n",
    "```bash\n",
    "sos run myscript -q shark\n",
    "```\n",
    "\n",
    "would send all tasks in workflow `default` defined in `myscript.sos` to a task queue `shark`. Note that this option does not override option `queue` of steps so you could send some tasks to specific queues and all others to the default queue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `dryrun` mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "The dryrun running mode is very useful in checking if your scripts are correctly translated and if your machine settings are correct. **It is strongly recommended that you execute your script in dryrun mode before submitting them to remote hosts and/or task queues**.\n",
    "\n",
    "How tasks are executed depends a bit on your configuration but basically,\n",
    "\n",
    "1. For local tasks (`process` task engine), the tasks are executed directly with `sos execute task -n` (`-n` for dryrun mode).\n",
    "\n",
    "2. For direct remote execution (e.g. `sos run script -q server -n` with `queue_type` set to `process` (default)), the tasks will be generated and copied to the remote server, and will be executed with command `sos execute task -n`.\n",
    "\n",
    "3. For submitting to a PBS task queue (e.g. `sos run script -q pbs -n` with `queue_type` set to `pbs`), the tasks will be generated, copied to remote host. Job files will also be generated according to `job_template` and will be copied to the remote host. However, instead of using `submit_cmd` to submit the job to the PBS queue, **the job script will be executed directly on the head node**. It is therefore important for you to allow the jobs to be run in `dryrun` mode and complete in seconds. Otherwise your system admin would hunt you for running large jobs directly on head nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Dryrun mode allows you to test your workflow on a new server or task queue with the following approach:\n",
    "\n",
    "1. Define a `host_test` queue that is identical to `host` but with `max_running_jobs` set to 1.\n",
    "2. Run the script in dryrun mode, which would perform all file synchronization, task translation, submission steps, and execution steps. The only difference is that the scripts are printed instead of executed.\n",
    "    ```\n",
    "    sos run script -q host_test -n\n",
    "    ```\n",
    "    kill the command with `Ctrl-C` if the scripts are correct.\n",
    "    \n",
    "3. Submit one job to the task queue using command\n",
    "    ```\n",
    "    sos run script -q host_test\n",
    "    ```\n",
    "   kill the command if the task gets started and runs correctly.\n",
    "4. Submit the rest of the tasks using command\n",
    "    ```\n",
    "    sos run script -q host\n",
    "    ```\n",
    "    The job submitted at step 3 will be recognized and monitored. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Command `sos status`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command \n",
    "\n",
    "```bash\n",
    "sos status [tasks] -q query\n",
    "```\n",
    "checkes the status of tasks. You can specify any number of first characeters of a task to specify a task, for example,\n",
    "\n",
    "```bash\n",
    "sos task 7\n",
    "sos task 77e\n",
    "sos task 7736e\n",
    "```\n",
    "would all work for a task with ID `77e36e7404cf6c2ef7079a09e84a4d6d`, but multiple tasks could be identifies if they share the same leading digits. Actually, \n",
    "\n",
    "```\n",
    "sos task \n",
    "```\n",
    "would match all tasks and list the status of all local tasks.\n",
    "\n",
    "Option `-q` specifies the task queue to monitor. For example,\n",
    "\n",
    "```\n",
    "sos status -q docker\n",
    "```\n",
    "\n",
    "would check the status of all tasks on a remote host `docker`. You can monitor the tasks on `docker` on any machine with defined host `docker`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "###  `sos status -v -q`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Option `-v` controls the details of the output of command `sos status`. For example,\n",
    "\n",
    "```\n",
    "sos status e7404cf6c2 -v0\n",
    "```\n",
    "would print just the status of the task (e.g. `running`).\n",
    "\n",
    "```\n",
    "sos task 77e -v1\n",
    "```\n",
    "would print the task id and their status\n",
    "\n",
    "```\n",
    "77e36e7404cf6c2ef7079a09e84a4d6d    running\n",
    "77e3c2ef7079a236e7404cf6c2f343d3    completed\n",
    "```\n",
    "\n",
    "Option `-v0` and `-v1` could check the status of multiple tasks, as realized by SoS. Some tasks queues have their own task status command and option `-v2` (and upper) will use these commands (if specified) to check the status of the jobs. That is to say\n",
    "\n",
    "``` bash\n",
    "sos task 77e36 -v2\n",
    "```\n",
    "\n",
    "might return output of a command\n",
    "\n",
    "```\n",
    "qstat 18433\n",
    "```\n",
    "\n",
    "if the task has been submitted to a cluster named `shark` with a job id `18433`.\n",
    "\n",
    "If you would like to know more about the tasks,\n",
    "\n",
    "```bash\n",
    "sos task 77e36 -v3\n",
    "```\n",
    "\n",
    "would list the script the task is running and all variables in abbreviated format, and\n",
    "\n",
    "```bash\n",
    "sos task 77e36 -v4\n",
    "```\n",
    "would list all variables in complete form.\n",
    "\n",
    "Finally, using `-q` in combination with `-v` allows you to list the variables used in remote server.\n",
    "\n",
    "```bash\n",
    "sos task 77e36 -v4 -q linux\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `sos kill` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Command\n",
    "\n",
    "```bash\n",
    "sos kill [tasks] [-q queue]\n",
    "```\n",
    "\n",
    "kills specified or all tasks on specified job queue `queue`. Because the same job could be executed on different queues (you have have done so), you will have to specify the correct queue name to kill the job on different queues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `sos execute`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Command \n",
    "````\n",
    "sos execute [tasks] [-q queue]\n",
    "```\n",
    "\n",
    "is the command that is used internally by `sos run` to execute tasks but you could use this command to execute tasks externally. For example, if a task failed on a server, you could use command\n",
    "\n",
    "```\n",
    "sos execute task_id -q server\n",
    "```\n",
    "\n",
    "to execute the command on another server. Note that `task_id` specifies a local task with local paths. The task will be converted to a remote task (with path names converted for that host) if `server` specifies a remote host. This makes it easy for you to re-submit tasks to the same server after changing server configuration, or submit the same task to a different server. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "output_cache": "[]"
   },
   "source": [
    "### Remote execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### PBS Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Host configuration\n",
    "\n",
    "`~/.sos/hosts.yml`\n",
    "\n",
    "```\n",
    "hosts:\n",
    "  nautilus:\n",
    "    address: nautilus.mdanderson.edu\n",
    "    paths:\n",
    "      home: /scratch/bcb/bpeng1\n",
    "    queue_type: pbs\n",
    "    status_check_interval: 30\n",
    "    job_template: |\n",
    "      #!/bin/bash\n",
    "      #PBS -N ${job_name}\n",
    "      #PBS -l nodes=1:ppn=${cores}\n",
    "      #PBS -l walltime=${walltime//3600}:${walltime/60%60}:${walltime%60}\n",
    "      #PBS -l mem=${mem//1000000}GB\n",
    "      #PBS -o ${cur_dir}/${task}.out\n",
    "      #PBS -e ${cur_dir}/${task}.err\n",
    "      #PBS -m ae\n",
    "      #PBS -M bpeng@mdanderson.org\n",
    "      #PBS -v ${cur_dir}\n",
    "      cd ${cur_dir}\n",
    "      sos execute ${task} -v ${verbosity} -s ${sig_mode} \\\n",
    "        ${'--dryrun' if run_mode=='dryrun' else ''}\n",
    "    max_running_jobs: 100\n",
    "    submit_cmd: msub ${job_file}\n",
    "    status_cmd: qstat ${job_id}\n",
    "    kill_cmd: qdel ${job_id}\n",
    "  workstation:\n",
    "    paths:\n",
    "      home:/Users/bpeng1\n",
    "```\n",
    "\n",
    "`~/.sos/config.yml`\n",
    "\n",
    "```\n",
    "localhost: workstation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test script\n",
    "\n",
    "```\n",
    "[10]\n",
    "input: for_each={'tid': range(10) }\n",
    "\n",
    "task: concurrent=True, walltime='00:20:00', mem='100M', cores=1\n",
    "\n",
    "run:\n",
    "    echo I am task ${tid}\n",
    "    sleep ${60  * (tid + 1)}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "output_cache": "[]"
   },
   "source": [
    "#### Commands\n",
    "\n",
    "```\n",
    "sos run test -q nautilus\n",
    "sos status -q nautilus\n",
    "sos kill cb1 -q nautilus\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "It can be tricky to get everything working but you only need to do this once for each server or task queue that you would like to use. After that, you are free to submit your tasks to any of the servers without worrying about different file systems, task queues etc. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Select cell kernel",
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos.jupyter.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "celltoolbar": true,
   "kernels": [
    [
     "sos",
     "SoS",
     ""
    ],
    [
     "bash",
     "Bash",
     "#E6EEFF"
    ],
    [
     "python3",
     "Python3",
     "#EAFAF1"
    ],
    [
     "python2",
     "Python2",
     "#F6FAEA"
    ],
    [
     "ir",
     "R",
     "#FDEDEC"
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0,
    "style": "side"
   }
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "172px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "675px",
    "left": "0px",
    "right": "1920px",
    "top": "106px",
    "width": "361px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
